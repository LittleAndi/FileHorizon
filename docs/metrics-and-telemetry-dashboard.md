Based on the current FileHorizon implementation, this document marks which metrics/telemetry are already provided by the backend and where the UI can safely visualize them. Legend: âœ… covered â€¢ ğŸŸ¨ partially covered/derivable â€¢ âŒ not covered (needs backend or external source).

## **ğŸ“Š Core System Metrics**

### **1. Active Transfers** ğŸŸ¨

- What we have: Traces for in-flight work (spans: `file.process`, `file.orchestrate`, `reader.open`, `sink.write`).
- UI can derive â€œactive nowâ€ from currently-open spans. There is no direct gauge metric yet.
- Rewrite suggestion: â€œActive transfers (derived from active spans)â€.

### **2. Queue Depth** ğŸŸ¨

- What we have: Counters `queue.enqueued`, `queue.dequeued`, `queue.enqueue.failures`, `queue.dequeue.failures` and spans `queue.enqueue`, `queue.dequeue`.
- Depth itself (stream length) isnâ€™t emitted yet; can be added as a gauge. Trends are derivable from counters in the meantime.
- Rewrite suggestion: â€œQueue activity (enqueued/dequeued rate); add stream length gauge for depth.â€

### **3. Success Rate** âœ…

- What we have: Counters `files.processed`, `files.failed` (protocol-tagged). UI can compute percentage over a 24h window.

### **4. System Load** âŒ

- Not emitted by the app. Use container/node telemetry (e.g., cAdvisor/Node Exporter/Prometheus).

---

## **ğŸ”Œ Protocol-Specific Metrics**

### **5. UNC Sources** ğŸŸ¨

- What we have: Local polling emits discoveries via `files.discovered`; unstable files via `files.skipped.unstable`; failures via `poll.source.errors` (shared with remote).
- Online/offline source counts arenâ€™t tracked; a per-source health gauge would complete this.

### **6. FTP/SFTP Servers** ğŸŸ¨

- What we have: Remote poll spans (`poll.remote.cycle`, `poll.remote.source`) and `poll.source.errors` counter. Discovery flows increment `files.discovered`.
- Connected vs total would benefit from a connect_success/connect_failure metric per source (tagged by source name).

### **7. Azure Service Bus** âŒ

- Not applicable in current codebase.

---

## **ğŸ“ File Transfer Details (Table)**

### **8. Per-Transfer Metrics** âœ… (with notes)

- Covered fields from backend:
  - Transaction ID â†’ `FileEvent.Id`
  - Filename/Source path â†’ `FileEvent.Metadata.SourcePath`
  - Destination path â†’ `FileEvent.DestinationPath`
  - Protocol â†’ `FileEvent.Protocol`
  - File size â†’ `FileEvent.Metadata.SizeBytes`
  - Start time â†’ `FileEvent.DiscoveredAtUtc` (plus span start times)
  - Duration â†’ `processing.duration.ms` histogram and span durations
- Status mapping:
  - processing/completed/error/queued can be inferred from spans and `files.processed`/`files.failed` counters.
- Progress percentage:
  - Not emitted; would require per-file progress or periodic write metrics. Could estimate if we add per-file bytes copied and total size.

---

## **âš¡ Performance Metrics**

### **9. Throughput** âœ…

- What we have: `bytes.copied` counter (sink-level) and `processing.duration.ms` histogram. Throughput can be computed over time windows.
- Enhancement: tag `bytes.copied` by `sink.name` and `file.protocol` for breakdowns.

### **10. Container Memory** âŒ

- Not emitted by the app. Use container runtime metrics.

### **11. Redis Streams Health** ğŸŸ¨

- What we have: `queue.enqueue.failures`, `queue.dequeue.failures` counters, and queue spans. Connection quality and lag gauges not yet emitted.
- Rewrite suggestion: â€œQueue health (enqueue/dequeue failures and activity rate)â€. Add a simple health/latency gauge for richer status.

---

## **ğŸ“ Recent Events (Activity Log)**

### **12. Event Stream** âœ…

- Success: green
- Error: red (pulsing)
- Info: blue
- What we have: Rich traces (OTLP) for processing and polling stages, plus logs. UI can map span status to visual indicators.

---

## **ğŸ“ˆ Additional Context Metrics**

These are implied or supporting metrics that enhance the dashboard:

13. **Trend indicators** âœ… (derive from counters/histograms over time windows)
14. **Status badges** âœ… (drive from thresholds on processed/failed, poll errors, queue failures)
15. **Time-based filtering** âœ… (metrics are time-series; use 24h windows in dashboards)
16. **Protocol distribution** âœ… (protocol-tagged counters/histograms)
17. **Error rate tracking** âœ… (from `files.failed` vs `files.processed`)
18. **Real-time vs historical** âœ… (metrics + traces support both views)

---

## **ğŸ¯ Summary**

The dashboard tracks **11 primary metric categories** covering:

- System health (4 core metrics)
- Protocol connectivity (3 protocol groups)
- Individual transfer tracking (detailed table)
- Performance monitoring (3 infrastructure metrics)
- Event logging (activity stream)

All metrics support real-time updates, trend analysis, and visual status indicators to give operators comprehensive visibility into the FileHorizon MFT system.

---

## ğŸ“¦ Current instrumentation inventory (for dashboard bindings)

- Counters
  - `files.processed`, `files.failed`
  - `bytes.copied`
  - `queue.enqueued`, `queue.dequeued`, `queue.enqueue.failures`, `queue.dequeue.failures`
  - `poll.cycles`, `poll.source.errors`, `files.discovered`, `files.skipped.unstable`
- Histograms
  - `processing.duration.ms`, `poll.cycle.duration.ms`
- Spans (ActivitySource: `FileHorizon.Pipeline`)
  - `pipeline.lifetime`
  - `file.process` (FileProcessingService)
  - `file.orchestrate` (Orchestrator)
  - `reader.open`, `sink.write`
  - `poll.remote.cycle`, `poll.remote.source`
  - `queue.enqueue`, `queue.dequeue`

## ğŸ”§ Suggested backend extensions (optional)

- Active transfers gauge: observable that tracks in-flight transfers (inc on start, dec on finish).
- Queue depth gauge: report Redis Stream length(s) for precise backlog.
- Source connectivity: counters/gauges for connect_success/connect_failure per source (tagged by source name).
- Throughput breakdowns: tag `bytes.copied` with `sink.name` and `file.protocol` for richer PromQL slices.

---

## ğŸ§© Optional next steps (implementation checklist)

These items are not required for the current dashboard but can be added incrementally to enrich insights:

- [ ] Active transfers gauge
  - Backend: Add an observable up/down counter or gauge that increments on `file.orchestrate` start and decrements on completion/error.
  - UI: Show â€œActive nowâ€ count and sparkline.
- [ ] Queue depth gauge
  - Backend: Periodically emit Redis Stream length per queue as a gauge (tags: queue name, consumer group).
  - UI: Display current backlog and alert on thresholds.
- [ ] Source connectivity metrics
  - Backend: Emit `connect_success` / `connect_failure` counters per source (tag: source name, protocol); optionally a last-success timestamp gauge.
  - UI: Show connected vs total sources and recent failures.
- [ ] Throughput breakdown tags
  - Backend: Tag `bytes.copied` with `sink.name` and `file.protocol` for per-sink/protocol slices.
  - UI: Add stacked area/bar charts by sink/protocol.

Note: Each enhancement should include minimal unit tests (instrument existence and basic tag shape) and can be rolled out independently.
